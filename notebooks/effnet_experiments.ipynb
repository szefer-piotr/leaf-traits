{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My implementation of the EfficientNetVit\n",
    "\n",
    "data_setup.py - create datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import timm\n",
    "import imageio.v3 as imageio\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import albumentations as A\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "N_VAL_SAMPLES0 = 4096\n",
    "TESTING = True\n",
    "MODEL_NAME = 'efficientnet_b5.sw_in12k_ft_in1k'\n",
    "V_MIN = 0\n",
    "V_MAX = 0\n",
    "TARGET_COLUMNS_TEST = ['X4', 'X11', 'X18', 'X50', 'X26', 'X3112']\n",
    "BATCH_SIZE = 24\n",
    "BATCH_SIZE_VAL = 128\n",
    "LOG_FEATURES = ['X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean'] # x4 not here, X4 doesnt need log-scaling?\n",
    "N_TARGETS = len(TARGET_COLUMNS)\n",
    "\n",
    "# Feature Scaler\n",
    "Y_SHIFT = np.zeros(N_TARGETS)\n",
    "Y_STD = np.zeros(N_TARGETS)\n",
    "\n",
    "IMAGE_SIZE0 = 512\n",
    "IMAGE_SIZE = 288\n",
    "\n",
    "# # Dataset\n",
    "# RECOMPUTE_DATAFRAMES = False\n",
    "# # Training\n",
    "# LR_MAX = 3e-4\n",
    "# WEIGHT_DECAY = 0.01\n",
    "N_EPOCHS = 12\n",
    "# TRAIN_MODEL = True\n",
    "# # Others\n",
    "# # IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n",
    "# IS_INTERACTIVE = False\n",
    "# SEED = 42\n",
    "# EPS = 1e-6\n",
    "# EPS_CUDA = torch.tensor([EPS]).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0 = pd.read_pickle('train.pkl')\n",
    "\n",
    "# Split train in train/val\n",
    "train, val = train_test_split(train0, test_size=N_VAL_SAMPLES0, shuffle=True, random_state=42)\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "\n",
    "test = pd.read_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use 5% of data for testing\n",
    "if TESTING:\n",
    "    testing_train_size = math.floor(train.shape[0]*0.05)\n",
    "    testing_val_size = math.floor(val.shape[0]*0.05)\n",
    "    testing_test_size = math.floor(test.shape[0]*0.05)\n",
    "    train = train.sample(testing_train_size)\n",
    "    val = val.sample(testing_val_size)\n",
    "    test = test.sample(testing_test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2569, 178), (204, 178), (327, 166))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.355158</td>\n",
       "      <td>0.897844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007290</td>\n",
       "      <td>45.738037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006107</td>\n",
       "      <td>25.881885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.060027</td>\n",
       "      <td>4.230211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001201</td>\n",
       "      <td>960.389841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.024205</td>\n",
       "      <td>20718.088466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0             1\n",
       "0 -1.355158      0.897844\n",
       "1  0.007290     45.738037\n",
       "2  0.006107     25.881885\n",
       "3  0.060027      4.230211\n",
       "4  0.001201    960.389841\n",
       "5  0.024205  20718.088466"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_MIN = train[TARGET_COLUMNS].quantile(0.0005)\n",
    "V_MAX = train[TARGET_COLUMNS].quantile(0.985)\n",
    "pd.DataFrame(zip(V_MIN, V_MAX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_FEATURES: 163\n"
     ]
    }
   ],
   "source": [
    "# Feature Columns\n",
    "FEATURE_COLUMNS = test.columns.values[1:-2]\n",
    "N_FEATURES = len(FEATURE_COLUMNS)\n",
    "print(f'N_FEATURES: {N_FEATURES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create masks with transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Filter outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(df, TARGET_COLUMNS, V_MIN, V_MAX):\n",
    "    '''\n",
    "    This function produces a boolean mask for target columns \n",
    "    based on value range between V_MIN and V_MAX values.\n",
    "    Returns a boolean vector indicating observations with True for all six traits falling within the V_MIN - V_MAX range.\n",
    "    '''\n",
    "    mask = np.empty(shape = df[TARGET_COLUMNS].shape, dtype=bool)\n",
    "    for idx, (t, v_min, v_max) in enumerate(zip(TARGET_COLUMNS, V_MIN, V_MAX)):\n",
    "        labels = df[t].values\n",
    "        mask[:, idx] = ((labels > v_min) & (labels < v_max))\n",
    "    return mask.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_TRAIN = get_mask(train, TARGET_COLUMNS, V_MIN, V_MAX)\n",
    "MASK_VAL = get_mask(val, TARGET_COLUMNS, V_MIN, V_MAX)\n",
    "train_mask = train[MASK_TRAIN].reset_index(drop=True)\n",
    "val_mask = val[MASK_VAL].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Add number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_TRAIN_SAMPLES: 2378 | N_VAL_SAMPLES: 195 | N_STEPS_PER_EPOCH: 99 | N_VAL_STEPS_PER_EPOCH: 2 | N_STEPS: 1189\n"
     ]
    }
   ],
   "source": [
    "# Add number of steps\n",
    "N_TRAIN_SAMPLES = len(train_mask)\n",
    "N_VAL_SAMPLES = len(val_mask)\n",
    "N_STEPS_PER_EPOCH = (N_TRAIN_SAMPLES // BATCH_SIZE)\n",
    "N_VAL_STEPS_PER_EPOCH = math.ceil(N_VAL_SAMPLES / BATCH_SIZE_VAL)\n",
    "N_STEPS = N_STEPS_PER_EPOCH * N_EPOCHS + 1\n",
    "print(f\"N_TRAIN_SAMPLES: {N_TRAIN_SAMPLES} | N_VAL_SAMPLES: {N_VAL_SAMPLES} | N_STEPS_PER_EPOCH: {N_STEPS_PER_EPOCH} | N_VAL_STEPS_PER_EPOCH: {N_VAL_STEPS_PER_EPOCH} | N_STEPS: {N_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Label normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_y(TARGET_COLUMNS, LOG_FEATURES, Y_SHIFT, Y_STD, df, normalize = False):\n",
    "    '''\n",
    "    Args:\n",
    "    y - an empty array\n",
    "    '''\n",
    "    y = np.zeros_like(df[TARGET_COLUMNS], dtype=np.float32)\n",
    "    for target_idx, target in enumerate(TARGET_COLUMNS):\n",
    "        v = df[target]\n",
    "        if normalize:\n",
    "            # Log10 transform\n",
    "            if target in LOG_FEATURES:\n",
    "                v = np.log10(v)\n",
    "            # Shift to have zero median\n",
    "            Y_SHIFT[target_idx] = np.mean(v)\n",
    "            v = v - np.median(v)\n",
    "            # Uniform variance\n",
    "            Y_STD[target_idx] = np.std(v)\n",
    "            v = v/np.std(v)\n",
    "        # Assign to y_train\n",
    "        y[:, target_idx] = v\n",
    "    return y \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mask_raw = fill_y(TARGET_COLUMNS, LOG_FEATURES, Y_SHIFT, Y_STD, train_mask, normalize=False)\n",
    "y_train_mask = fill_y(TARGET_COLUMNS, LOG_FEATURES, Y_SHIFT, Y_STD, train_mask, normalize=True)\n",
    "y_val_mask = fill_y(TARGET_COLUMNS, LOG_FEATURES, Y_SHIFT, Y_STD, train_mask, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_shift</th>\n",
       "      <th>y_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X4_mean</th>\n",
       "      <td>0.515449</td>\n",
       "      <td>0.140351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X11_mean</th>\n",
       "      <td>1.149262</td>\n",
       "      <td>0.224978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X18_mean</th>\n",
       "      <td>-0.035369</td>\n",
       "      <td>0.656117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X50_mean</th>\n",
       "      <td>0.172500</td>\n",
       "      <td>0.160889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X26_mean</th>\n",
       "      <td>0.392586</td>\n",
       "      <td>1.023566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X3112_mean</th>\n",
       "      <td>2.817999</td>\n",
       "      <td>0.670438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             y_shift     y_std\n",
       "X4_mean     0.515449  0.140351\n",
       "X11_mean    1.149262  0.224978\n",
       "X18_mean   -0.035369  0.656117\n",
       "X50_mean    0.172500  0.160889\n",
       "X26_mean    0.392586  1.023566\n",
       "X3112_mean  2.817999  0.670438"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Values\n",
    "display(pd.DataFrame({\n",
    "    'y_shift': Y_SHIFT,\n",
    "    'y_std': Y_STD\n",
    "}, index=TARGET_COLUMNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_distribution():\n",
    "    fig, axes = plt.subplots(N_TARGETS, 3, figsize=(20, N_TARGETS*4))\n",
    "    v_raw = train[TARGET_COLUMNS].values\n",
    "    for (ax_raw, ax_mask, ax_norm), target, v_r, v_n in zip(axes, TARGET_COLUMNS, v_raw.T, y_train_mask.T):\n",
    "        # Raw\n",
    "        ax_raw.hist(v_r, bins=128)\n",
    "        ax_raw.set_title(f'{target} Raw min: {v_r.min():.3f}, max: {v_r.max():.2e}, µ: {v_r.mean():.2e}, σ: {v_r.std():.2f}', size=10)\n",
    "        # Masked\n",
    "        v_m = v_r[MASK_TRAIN]\n",
    "        ax_mask.hist(v_r, bins=128)\n",
    "        ax_mask.set_title(f'{target} Masked min: {v_m.min():.3f}, max: {v_m.max():.2e}, µ: {v_m.mean():.2e}, σ: {v_m.std():.2f}', size=10)\n",
    "        # Normalized\n",
    "        ax_norm.hist(v_n, bins=128)\n",
    "        ax_norm.set_title(f'{target} Norm min: {v_n.min():.3f}, max: {v_n.max():.2f}, µ: {v_n.mean():.2f}, σ: {v_n.std():.2f}', size=10)\n",
    "    plt.subplots_adjust(hspace=0.25, wspace=0.30)\n",
    "    plt.show()\n",
    "    \n",
    "# plot_target_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Standardize the fratures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_SCALER = StandardScaler()\n",
    "\n",
    "# Fit and transform on training features\n",
    "train_features_mask = FEATURE_SCALER.fit_transform(train_mask[FEATURE_COLUMNS].values.astype(np.float32))\n",
    "\n",
    "# Transform val/test\n",
    "val_features_mask = FEATURE_SCALER.transform(val_mask[FEATURE_COLUMNS].values.astype(np.float32))\n",
    "test_features = FEATURE_SCALER.transform(test[FEATURE_COLUMNS].values.astype(np.float32))\n",
    "\n",
    "# Convert features to torch tensors\n",
    "train_features_mask = torch.tensor(train_features_mask)\n",
    "val_features_mask = torch.tensor(val_features_mask)\n",
    "test_features = torch.tensor(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Transforms and augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to joint augmentations and model-specific transformations\n",
    "# TRAIN_AUGMENTATIONS = A.Compose([\n",
    "#     A.RandomSizedCrop(\n",
    "#         [int(0.85*IMAGE_SIZE), IMAGE_SIZE],\n",
    "#         IMAGE_SIZE0, IMAGE_SIZE0, w2h_ratio=1.0, p=1\n",
    "# \t),\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p = 0.5),\n",
    "#     A.ImageCompression(quality_lower=75, quality_upper=100, p = 0.5),\n",
    "#     # Model specific transformations\n",
    "#     A.Resize(448, 448),\n",
    "#     A.Normalize(mean=(0.4850, 0.4560, 0.4060), std=(0.2290, 0.2240, 0.2250)),\n",
    "# \tToTensorV2(),\n",
    "# ])\n",
    "\n",
    "# VAL_TEST_TRANSFORMS = A.Compose([\n",
    "#     # Model specific transformations\n",
    "#     A.Resize(448, 448),\n",
    "#     A.Normalize(mean=(0.4850, 0.4560, 0.4060), std=(0.2290, 0.2240, 0.2250)),\n",
    "#     ToTensorV2(),\n",
    "# ])\n",
    "\n",
    "# Training Augmentations\n",
    "TRAIN_TRANSFORMS = A.Compose([\n",
    "        A.RandomSizedCrop(\n",
    "            [int(0.85*IMAGE_SIZE0), IMAGE_SIZE0],\n",
    "            IMAGE_SIZE, IMAGE_SIZE, w2h_ratio=1.0, p=1.0\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.50),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.10, contrast_limit=0.10, p=0.50),\n",
    "        A.ImageCompression(quality_lower=75, quality_upper=100, p=0.5),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "# Test Augmentations\n",
    "VAL_TEST_TRANSFORMS = A.Compose([\n",
    "        A.Resize(IMAGE_SIZE,IMAGE_SIZE),\n",
    "        ToTensorV2(),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create model specific transformations\n",
    "\n",
    "These model specific transofrmations need to be applied to all train, validation and test data. These are basic transformations that need to be applied for a given model to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b5.sw_in12k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b5.sw_in12k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(448, 448), interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(448, 448))\n",
       "    ToTensor()\n",
       "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model(MODEL_NAME, pretrained=True)\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "model = model.eval()\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "image_transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "image_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I mixed the two: `albumentations` and `torchvidsion.Compose`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(image_transforms), type(TRAIN_AUGMENTATIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torchvision` transformations work if I use `Image.fromarray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out the transformations\n",
    "\n",
    "The data from the dataset is in the form of `bytes`. In the dataset the resulting `\"image'` element of the result list is a `torch.Tensor`. \n",
    "I want to:\n",
    "1. Transform `bytes` to `nd.array`\n",
    "2. Augment `nd.array` and return `torch.Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 448\n",
    "img_bytes = train_mask['jpeg_bytes'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_specific_transforms(transforms, image_bytes):\n",
    "       img = Image.fromarray(imageio.imread(image_bytes))\n",
    "       trans_img = transforms(img)\n",
    "       print(f\"Image spec trans: {trans_img.shape}\")\n",
    "       return trans_img\n",
    "\n",
    "def augment(aug_pipeline, image):\n",
    "       image_array = np.array(image.permute(1,2,0))\n",
    "       print(f\"Permuted shape: {image_array.shape}\")\n",
    "       augmented_img = aug_pipeline(image=image_array)['image']\n",
    "       print(f\"Augmented shape: {augmented_img.shape}\")\n",
    "       return augmented_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now model specific transformation will be included in the dataloader.\n",
    "In the model I will use augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 448, 448])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment(TRAIN_AUGMENTATIONS, model_specific_transforms(image_transforms, img_bytes)).shape\n",
    "# imageio.imread(self.X_jpeg_bytes[index])\n",
    "# mst = model_specific_transforms(image_transforms, image_bytes=img_bytes)\n",
    "# There is a problem with the rang of the RGB\n",
    "# plt.imshow(mst.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'albumentations.core.composition.Compose'> <class 'torchvision.transforms.transforms.Compose'>\n"
     ]
    }
   ],
   "source": [
    "print(type(TRAIN_AUGMENTATIONS), type(image_transforms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# Create the Plant Traits Photo and Tabular Dataset\n",
    "class PTFTDataset(Dataset):\n",
    "\tdef __init__(self, X_jpeg_bytes, y, features, transforms=None, augmentations=None):\n",
    "\t\tself.X_jpeg_bytes = X_jpeg_bytes\n",
    "\t\tself.y = y\n",
    "\t\tself.features = features\n",
    "\t\tself.transforms = transforms\n",
    "\t\tself.augmentations = augmentations\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.X_jpeg_bytes)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tX_sample = {\n",
    "\t\t\t'image': augment(self.augmentations, model_specific_transforms(self.transforms,\n",
    "\t\t\t\tself.X_jpeg_bytes[index])),\n",
    "\t\t\t'feature': self.features[index],\n",
    "\t\t}\n",
    "\t\ty_sample = self.y[index]\n",
    "\n",
    "\t\treturn X_sample, y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloder function\n",
    "def create_dataloaders(\n",
    "\ttrain_data: pd.DataFrame,\n",
    "\ttrain_y: torch.tensor,\n",
    "\ttrain_features: torch.tensor,\n",
    "\tval_data: pd.DataFrame,\n",
    "\tval_y: torch.tensor,\n",
    "\tval_features: torch.tensor,\n",
    "\ttest_data: pd.DataFrame,\n",
    "\ttest_y: torch.tensor,\n",
    "\ttest_features: torch.tensor,\n",
    "\ttransforms: transforms.Compose,\n",
    "\tbatch_size: int,\n",
    "\tnum_workers: int=0\n",
    "):\n",
    "\t\n",
    "\t# Train dataset and dataloader\n",
    "\ttrain_dataset = PTFTDataset(\n",
    "\t\ttrain_data['jpeg_bytes'].values,\n",
    "\t\ttrain_y,\n",
    "\t\ttrain_features,\n",
    "\t\ttransforms=transforms,\n",
    "\t\taugmentations=TRAIN_AUGMENTATIONS\n",
    "\t)\n",
    "    \n",
    "\ttrain_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\t\n",
    "\t# Validataion dataset and dataloader\n",
    "\tval_dataset = PTFTDataset(\n",
    "\t\tval_data['jpeg_bytes'].values,\n",
    "\t\tval_y,\n",
    "\t\tval_features,\n",
    "\t\ttransforms=transforms\n",
    "\t)\n",
    "    \n",
    "\tval_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\t\n",
    "\t# Test dataset\n",
    "\ttest_dataset = PTFTDataset(\n",
    "\t\ttest_data['jpeg_bytes'].values,\n",
    "\t\ttest_y,\n",
    "\t\ttest_features,\n",
    "\t\ttransforms=transforms\n",
    "\t)\n",
    "\n",
    "\treturn train_dataloader, val_dataloader, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Use autotransforms in dataloaders and augmentations in the model or during trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x221812e5c30>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x221812e5d50>,\n",
       " <__main__.PTFTDataset at 0x221812e6470>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataset = create_dataloaders(\n",
    "    train_data=train_mask,\n",
    "    train_y=y_train_mask,\n",
    "\ttrain_features=train_features_mask,\n",
    "\tval_data=val_mask,\n",
    "\tval_y=y_val_mask,\n",
    "\tval_features=val_features_mask,\n",
    "\ttest_data=test,\n",
    "\ttest_y=test['id'].values,\n",
    "\ttest_features=test_features,\n",
    "\ttransforms=image_transforms,\n",
    "\tbatch_size=BATCH_SIZE,\n",
    "\tnum_workers=0\n",
    ")\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_iter = iter(train_dataloader)\n",
    "# X_batch, y_batch = next(train_dataloader_iter)\n",
    "# for k, v in X_batch.items():\n",
    "#     print(f'X_batch {k} shape: {v.shape}, dtype: {v.dtype}')\n",
    "#     print(f'X_batch {k} min: {v.min():.3f}, max: {v.max():.3f}')\n",
    "#     print(f'X_batch {k} µ: {v.float().mean():.3f}, σ: {v.float().std():.3f}')\n",
    "# # Label\n",
    "# print(f'y_batch shape: {y_batch.shape}, dtype: {y_batch.dtype}')\n",
    "# print(f'y_batch min: {y_batch.min():.3f}, max: {y_batch.max():.3f}')\n",
    "# print(f'y_batch µ: {y_batch.mean():.3f}, σ: {y_batch.std():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\plant_traits\\venv\\lib\\site-packages\\albumentations\\augmentations\\functional.py:566: UserWarning: Image compression augmentation is most effective with uint8 inputs, float32 is used as input.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n",
      "Image spec trans: torch.Size([3, 448, 448])\n",
      "Permuted shape: (448, 448, 3)\n",
      "Augmented shape: torch.Size([3, 448, 448])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'image': tensor([[[[-2.1071, -2.1084, -2.1107,  ..., -2.1481, -2.1462, -2.1335],\n",
       "            [-2.1037, -2.1063, -2.1101,  ..., -2.1485, -2.1466, -2.1350],\n",
       "            [-2.1032, -2.1063, -2.1103,  ..., -2.1485, -2.1472, -2.1382],\n",
       "            ...,\n",
       "            [-2.1176, -2.1246, -2.1322,  ..., -2.1151, -2.1158, -2.1181],\n",
       "            [-2.1189, -2.1265, -2.1340,  ..., -2.1157, -2.1153, -2.1164],\n",
       "            [-2.1202, -2.1280, -2.1347,  ..., -2.1189, -2.1174, -2.1164]],\n",
       "  \n",
       "           [[-2.0237, -2.0256, -2.0284,  ..., -2.0604, -2.0587, -2.0466],\n",
       "            [-2.0209, -2.0238, -2.0280,  ..., -2.0609, -2.0593, -2.0482],\n",
       "            [-2.0210, -2.0244, -2.0289,  ..., -2.0609, -2.0600, -2.0515],\n",
       "            ...,\n",
       "            [-2.0276, -2.0351, -2.0436,  ..., -2.0257, -2.0263, -2.0286],\n",
       "            [-2.0282, -2.0366, -2.0449,  ..., -2.0251, -2.0249, -2.0263],\n",
       "            [-2.0295, -2.0377, -2.0452,  ..., -2.0273, -2.0263, -2.0257]],\n",
       "  \n",
       "           [[-1.7903, -1.7928, -1.7969,  ..., -1.8329, -1.8306, -1.8175],\n",
       "            [-1.7878, -1.7915, -1.7968,  ..., -1.8335, -1.8313, -1.8191],\n",
       "            [-1.7880, -1.7921, -1.7978,  ..., -1.8338, -1.8320, -1.8223],\n",
       "            ...,\n",
       "            [-1.8015, -1.8091, -1.8178,  ..., -1.8114, -1.8123, -1.8154],\n",
       "            [-1.8025, -1.8109, -1.8193,  ..., -1.8116, -1.8115, -1.8139],\n",
       "            [-1.8038, -1.8122, -1.8197,  ..., -1.8146, -1.8136, -1.8137]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.1098, -2.1098, -2.1097,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1095, -2.1095, -2.1095,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1094, -2.1094, -2.1094,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            ...,\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "  \n",
       "           [[-2.0253, -2.0253, -2.0252,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0251, -2.0251, -2.0250,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0250, -2.0250, -2.0250,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            ...,\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "  \n",
       "           [[-1.7910, -1.7910, -1.7909,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.7907, -1.7907, -1.7907,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.7906, -1.7906, -1.7905,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            ...,\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.1278, -2.1281, -2.1277,  ..., -2.1166, -2.1143, -2.1127],\n",
       "            [-2.1274, -2.1272, -2.1269,  ..., -2.1172, -2.1151, -2.1134],\n",
       "            [-2.1270, -2.1271, -2.1270,  ..., -2.1179, -2.1158, -2.1142],\n",
       "            ...,\n",
       "            [-2.1244, -2.1226, -2.1206,  ..., -2.1437, -2.1433, -2.1424],\n",
       "            [-2.1244, -2.1222, -2.1208,  ..., -2.1437, -2.1435, -2.1430],\n",
       "            [-2.1242, -2.1225, -2.1214,  ..., -2.1437, -2.1435, -2.1431]],\n",
       "  \n",
       "           [[-2.0334, -2.0340, -2.0341,  ..., -2.0284, -2.0260, -2.0243],\n",
       "            [-2.0335, -2.0334, -2.0333,  ..., -2.0290, -2.0268, -2.0251],\n",
       "            [-2.0334, -2.0335, -2.0334,  ..., -2.0298, -2.0276, -2.0259],\n",
       "            ...,\n",
       "            [-2.0353, -2.0344, -2.0330,  ..., -2.0592, -2.0590, -2.0585],\n",
       "            [-2.0353, -2.0339, -2.0332,  ..., -2.0591, -2.0593, -2.0590],\n",
       "            [-2.0351, -2.0342, -2.0337,  ..., -2.0591, -2.0593, -2.0591]],\n",
       "  \n",
       "           [[-1.8029, -1.8036, -1.8036,  ..., -1.7930, -1.7907, -1.7890],\n",
       "            [-1.8032, -1.8029, -1.8028,  ..., -1.7939, -1.7917, -1.7900],\n",
       "            [-1.8034, -1.8030, -1.8026,  ..., -1.7947, -1.7925, -1.7909],\n",
       "            ...,\n",
       "            [-1.8079, -1.8071, -1.8055,  ..., -1.8242, -1.8240, -1.8234],\n",
       "            [-1.8081, -1.8067, -1.8057,  ..., -1.8241, -1.8242, -1.8240],\n",
       "            [-1.8079, -1.8069, -1.8063,  ..., -1.8241, -1.8243, -1.8241]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[-2.1179, -2.1178, -2.1175,  ..., -2.1009, -2.1008, -2.1008],\n",
       "            [-2.1167, -2.1179, -2.1178,  ..., -2.1012, -2.1011, -2.1011],\n",
       "            [-2.1113, -2.1165, -2.1179,  ..., -2.1020, -2.1017, -2.1016],\n",
       "            ...,\n",
       "            [-2.1082, -2.1075, -2.1072,  ..., -2.1078, -2.1064, -2.1037],\n",
       "            [-2.1077, -2.1075, -2.1075,  ..., -2.1149, -2.1144, -2.1125],\n",
       "            [-2.1090, -2.1085, -2.1076,  ..., -2.1173, -2.1170, -2.1169]],\n",
       "  \n",
       "           [[-2.0356, -2.0356, -2.0357,  ..., -2.0182, -2.0182, -2.0182],\n",
       "            [-2.0333, -2.0356, -2.0357,  ..., -2.0182, -2.0182, -2.0182],\n",
       "            [-2.0228, -2.0322, -2.0356,  ..., -2.0182, -2.0182, -2.0182],\n",
       "            ...,\n",
       "            [-2.0192, -2.0185, -2.0184,  ..., -2.0263, -2.0247, -2.0215],\n",
       "            [-2.0186, -2.0184, -2.0184,  ..., -2.0334, -2.0328, -2.0308],\n",
       "            [-2.0199, -2.0192, -2.0183,  ..., -2.0357, -2.0354, -2.0353]],\n",
       "  \n",
       "           [[-1.8044, -1.8044, -1.8043,  ..., -1.7870, -1.7870, -1.7870],\n",
       "            [-1.8008, -1.8034, -1.8041,  ..., -1.7870, -1.7870, -1.7870],\n",
       "            [-1.7894, -1.7988, -1.8036,  ..., -1.7870, -1.7870, -1.7870],\n",
       "            ...,\n",
       "            [-1.8042, -1.8040, -1.8043,  ..., -1.8026, -1.7999, -1.7955],\n",
       "            [-1.8042, -1.8041, -1.8043,  ..., -1.8041, -1.8039, -1.8028],\n",
       "            [-1.8044, -1.8044, -1.8043,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.1099, -2.1101, -2.1104,  ..., -2.1295, -2.1298, -2.1301],\n",
       "            [-2.1091, -2.1094, -2.1098,  ..., -2.1292, -2.1295, -2.1297],\n",
       "            [-2.1083, -2.1085, -2.1090,  ..., -2.1285, -2.1286, -2.1289],\n",
       "            ...,\n",
       "            [-2.0958, -2.0961, -2.0963,  ..., -2.0988, -2.0986, -2.0985],\n",
       "            [-2.0955, -2.0960, -2.0961,  ..., -2.0990, -2.0987, -2.0985],\n",
       "            [-2.0953, -2.0959, -2.0959,  ..., -2.0991, -2.0989, -2.0985]],\n",
       "  \n",
       "           [[-2.0306, -2.0308, -2.0311,  ..., -2.0460, -2.0462, -2.0463],\n",
       "            [-2.0298, -2.0301, -2.0305,  ..., -2.0464, -2.0465, -2.0467],\n",
       "            [-2.0289, -2.0292, -2.0296,  ..., -2.0465, -2.0465, -2.0465],\n",
       "            ...,\n",
       "            [-2.0281, -2.0283, -2.0283,  ..., -2.0285, -2.0286, -2.0286],\n",
       "            [-2.0281, -2.0282, -2.0282,  ..., -2.0284, -2.0284, -2.0282],\n",
       "            [-2.0281, -2.0281, -2.0281,  ..., -2.0284, -2.0284, -2.0282]],\n",
       "  \n",
       "           [[-1.8021, -1.8024, -1.8027,  ..., -1.8136, -1.8139, -1.8137],\n",
       "            [-1.8014, -1.8017, -1.8021,  ..., -1.8141, -1.8141, -1.8139],\n",
       "            [-1.8005, -1.8008, -1.8012,  ..., -1.8141, -1.8138, -1.8136],\n",
       "            ...,\n",
       "            [-1.8075, -1.8078, -1.8080,  ..., -1.8079, -1.8080, -1.8081],\n",
       "            [-1.8074, -1.8076, -1.8078,  ..., -1.8079, -1.8079, -1.8079],\n",
       "            [-1.8074, -1.8076, -1.8077,  ..., -1.8079, -1.8079, -1.8077]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.1171, -2.1173, -2.1176,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1177, -2.1178, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            ...,\n",
       "            [-2.1179, -2.1179, -2.1177,  ..., -2.1179, -2.1178, -2.1178],\n",
       "            [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "            [-2.1179, -2.1177, -2.1177,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "  \n",
       "           [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0356, -2.0355, -2.0353,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            [-2.0350, -2.0347, -2.0343,  ..., -2.0357, -2.0357, -2.0357],\n",
       "            ...,\n",
       "            [-2.0182, -2.0184, -2.0187,  ..., -2.0184, -2.0184, -2.0183],\n",
       "            [-2.0184, -2.0185, -2.0187,  ..., -2.0183, -2.0183, -2.0182],\n",
       "            [-2.0185, -2.0182, -2.0183,  ..., -2.0182, -2.0182, -2.0182]],\n",
       "  \n",
       "           [[-1.8040, -1.8041, -1.8042,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.8043, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "            ...,\n",
       "            [-1.7908, -1.7911, -1.7905,  ..., -1.7960, -1.7959, -1.7958],\n",
       "            [-1.7915, -1.7913, -1.7906,  ..., -1.7957, -1.7955, -1.7954],\n",
       "            [-1.7915, -1.7908, -1.7900,  ..., -1.7952, -1.7951, -1.7951]]]]),\n",
       "  'feature': tensor([[ 0.7615,  2.8778,  0.6707,  ..., -0.8512, -0.8620, -0.8473],\n",
       "          [-0.4953, -0.5367, -0.9660,  ...,  0.3752,  0.2220,  0.0782],\n",
       "          [ 0.5926,  0.3644, -0.3672,  ...,  0.2826,  0.3780,  0.3891],\n",
       "          ...,\n",
       "          [-0.2943, -0.0985, -0.5317,  ...,  0.6936,  0.3872,  0.3859],\n",
       "          [ 1.0560, -1.0641, -0.8537,  ..., -2.0881, -2.1761, -2.2213],\n",
       "          [ 0.7323,  0.9521,  0.1654,  ...,  1.6445,  1.8223,  1.9552]])},\n",
       " tensor([[-0.6397, -1.9825,  0.5799,  1.9542,  0.3059,  0.5672],\n",
       "         [-0.6351,  2.0645, -0.9177, -2.3956, -1.6992, -3.0047],\n",
       "         [ 0.5786, -0.0608,  2.1157,  0.7531,  2.0134,  1.6930],\n",
       "         [-0.5405, -0.4006, -0.7155, -0.6291, -0.3233, -0.1770],\n",
       "         [-1.0299, -0.0992,  2.1115,  0.9631,  1.7543,  1.8242],\n",
       "         [-0.2272,  0.4933, -0.3574,  0.4464, -1.1010,  0.1661],\n",
       "         [ 0.6329,  0.0733, -0.2427,  1.5064,  0.5612,  0.0098],\n",
       "         [-0.5739, -0.0701,  1.9079, -0.0928, -0.6451,  1.3160],\n",
       "         [ 0.9200, -1.1962,  1.5767,  0.7128,  1.8381,  1.1067],\n",
       "         [-0.3307,  0.6697,  0.2374,  0.1640,  1.1723,  0.7313],\n",
       "         [ 1.2699, -1.1836, -0.6265,  0.3775, -1.4117, -0.3697],\n",
       "         [-0.8458,  0.6146,  0.0288, -0.2259,  0.9334,  0.6054],\n",
       "         [-1.6819, -0.8272, -0.5751,  1.5887, -0.1962, -0.5023],\n",
       "         [-0.3685,  1.0211, -0.3184, -1.4212, -1.1966, -0.4829],\n",
       "         [ 0.6258,  0.4610,  0.1111, -1.1358, -1.9481, -0.6081],\n",
       "         [-0.0596, -1.5293, -0.1808,  1.6823, -0.3098, -1.1871],\n",
       "         [ 0.3290,  0.8120, -0.7373, -0.3434, -1.6914, -1.9481],\n",
       "         [ 0.9531, -0.8945,  1.2901,  1.1284,  0.8391,  1.5057],\n",
       "         [-1.5638,  0.8463, -0.1940,  0.3587, -0.1037,  0.3735],\n",
       "         [ 0.5608,  0.0673,  1.5500, -0.1786,  1.0990,  1.3528],\n",
       "         [ 0.0052, -0.1741, -0.4532,  0.5212, -0.3637, -1.1061],\n",
       "         [ 0.9961, -1.3968,  1.4607, -0.8994,  0.7251,  0.7469],\n",
       "         [ 0.5093, -1.3594, -0.4745,  1.7594, -0.1179, -2.6135],\n",
       "         [-0.3700,  0.2062, -0.7133, -0.3146, -1.3807, -0.6346]])]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataloader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 3, 448, 448])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch = next(train_dataloader_iter)[0]['image'].shape\n",
    "example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\tdef __init__(self,model_name=MODEL_NAME, augmentations = TRAIN_AUGMENTATIONS):\n",
    "\t\t\n",
    "\t\tsuper().__inti__()\n",
    "\t\t\n",
    "\t\tself.augmentations = augmentations\n",
    "\t\tself.backbone = timm.create_model(model_name,pretrained=True, num_classes=0)\n",
    "\t\tself.transforms = None\n",
    "\t\tself.features = nn.Sequential(\n",
    "\t\t\tnn.Linear(N_FEATURES, 256),\n",
    "\t\t\tnn.GELU(),\n",
    "\t\t\tnn.Linear(256, 256),\n",
    "\t\t)\n",
    "\t\tself.label = nn.Sequential(\n",
    "\t\t\tnn.Linear(256, 256),\n",
    "\t\t\tnn.GELU(),\n",
    "\t\t\tnn.Linear(256, N_TARGETS, bias=False),\n",
    "\t\t)\n",
    "\n",
    "\t\tself.initialize_weights()\n",
    "\n",
    "\tdef initialize_weights(self):\n",
    "\t\tnn.init.kaiming_uniform_(self.features[2].weight)\n",
    "\t\tnn.init.zeros_(self.label[2].weight)\n",
    "\n",
    "\tdef forward(self, inputs):\n",
    "\t\treturn {'label': self.label(\n",
    "\t\t\tself.backbone(self.augmentations(inputs['image'].float())) + self.features(inputs['frature'])\n",
    "\t\t)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
